This project is an implementation of the deep Q network described by deepmind in \href{https://arxiv.org/abs/1312.5602}{\tt their article of 2013} using python 3.\+5 and Theano

~ \subsection*{Dependencies}

\subsubsection*{Main dependencies}

The following dependencies are crucial for this project and are used by the agent itself. The given version should be used. However, higher version would probaly work too
\begin{DoxyItemize}
\item {\bfseries Arcade learning environment (A\+LE)}\+: the versions 0.\+5.\+0 and 0.\+5.\+1 downloadable on the \href{http://www.arcadelearningenvironment.org/downloads/}{\tt A\+LE website} don\textquotesingle{}t support python 3. However the current development version (December 2016) available on their \href{https://github.com/mgbellemare/Arcade-Learning-Environment}{\tt Github repository} does.
\item {\bfseries Theano}\+: The release version 0.\+8 and the current 0.\+9 development version (December 2016) seems to differ slightly as some packages/modules seems to have changed names or been moved. I used the 0.\+9 develpment version available from the \href{https://github.com/Theano/Theano}{\tt Theano Github repository}.
\item {\bfseries cv2}\+: The cv2 module is used to resize the images from A\+LE to feed the network used by the agent. The version used is the 3.\+1.\+0
\end{DoxyItemize}

\subsubsection*{Other dependencies}

These dependencies are used to save the agent, display the game or plot some statistics.
\begin{DoxyItemize}
\item {\bfseries Py\+Qt}\+: Py\+Qt version 4.\+8.\+7 is used to display the game and to build the window where the plots are displayed.
\item {\bfseries pyqtgraph}\+: The plots and percentile plots are displayed thanks to pyqtgraph version 0.\+10.\+0.
\item {\bfseries sqlite3}\+: The agent\textquotesingle{}s data and the statistics collected during the training are saved in an sqlite database.
\item {\bfseries matplotlib}\+: matplotlib is not really used and is there for debug purpose only. This should be removed in the future
\item {\bfseries Usual python modules}\+: collections, datetime, enum, gc, json, math, numbers, numpy, pickle, random, sys, time, threading
\end{DoxyItemize}

~ \subsection*{Usage}

To start training the agent, simply type {\ttfamily python Run.\+py $<$rom file$>$}. This will create a new default agent, initilize it and start training it. A window will open displaying the game in the form it\textquotesingle{}s fed to the agent and another windows will show the evolution of the agent accross the epochs. To stop the process, type {\ttfamily stop} and every processes will terminate.

The Atari 2600 R\+O\+Ms are available on the \href{http://www.atariage.com/system_items.html?SystemID=2600&ItemTypeID=ROM}{\tt Atari\+Age website}. After downloading the file you have to make sure it\textquotesingle{}s named as A\+LE expects it to be named otherwise, it can lead a segmentation fault. The names for each supported games can be found in the A\+LE sources, by inspecting the file related to your game in \href{https://github.com/mgbellemare/Arcade-Learning-Environment/tree/master/src/games/supported}{\tt /src/games/supported/}.

~ \subsection*{Results}

The agent is trained as explained in the \href{https://arxiv.org/abs/1312.5602}{\tt deepmind paper}.

Before training, a test set is built by picking random samples from games played randomly. By default, one epoch lasts 50000 iterations and every epoch, the agent plays for 10000 iterations using an epsilon greedy strategy with epsilon equal to 0.\+05. Every epoch, the following results are plotted and stored\+:
\begin{DoxyItemize}
\item The average value of the output of the network over the test set
\item The average value of the maximum outputs of the network over the test set
\item The average score per games played
\item The average reward per games played (as in the Deepmind\textquotesingle{}s paper, the reward are clipped between -\/1 and 1)
\end{DoxyItemize}

These results are stored in an sqlite database. \href{https://github.com/sqlitebrowser/sqlitebrowser}{\tt DB Browser for S\+Q\+Lite} provides an easy way to display and plot those results.

While I didn\textquotesingle{}t observe the same evolution of the output of the Q function as deepmind, I got similar results for the average score.

Have a look at the result folder for a analysis of the project.

~ \subsection*{References}

\mbox{[}1\mbox{]} \href{https://arxiv.org/abs/1312.5602}{\tt Playing Atari with Deep Reinforcement Learning}

\mbox{[}2\mbox{]} \href{https://github.com/mgbellemare/Arcade-Learning-Environment/blob/master/doc/manual/manual.pdf}{\tt Arcade Learning Environlent Technical Manual}

\mbox{[}3\mbox{]} \href{http://cs231n.github.io/}{\tt C\+S231n\+: Convolutional Neural Networks for Visual Recognition -\/ course notes} 